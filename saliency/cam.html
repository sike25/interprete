<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning Deep Features for Discriminative Localization (CAM)</title>
    <link rel="stylesheet" href="../paper.css">   
</head>
<body>
    <header>
        <h1>Learning Deep Features for Discriminative Localization (CAM)</h1>
        <div class="authors">Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba</div>
        <div class="publication">2015</div>

        <p>
            Visualization of the importance of input image portions for classifications, achieved by a network with only convolutional layers and topped with a global average pooling layer.
        </p>

        <a href="https://arxiv.org/abs/1512.04150" class="paper-link" target="_blank">https://arxiv.org/abs/1512.04150</a>
    </header>
    
    <main>

        <section>
            <h2>Related Work</h2>
            
            <h4>1. Weakly Supervised Object Localization:</h4>
            <p>
                This is an object detection task where the training image data has no bounding boxes.
            </p>
            <p>
                <ol type="a">
                    <li>Bergamo et al: Using masks to identify parts of images that maximize activation. (Sounds like RISE).</li>
                    
                    <li>Cinbis et al: Combining multiple instance learning with CNNs. Multiple instance learning is about having inputs that are bags of instances. Only one instance has to be positive to make the ground truth positive.</li>
                    
                    <li>Oquab et al: Overlapping patches and CNNs.</li>
                    
                    <li>Oquab et al 2: Global max pooling to pin point objects on an image (closest work).</li>
                </ol>
            </p>
            <p>
                Improvement: This paper replaces global max pooling with global average pooling because this captures the full extent of the objects, not just their boundaries and is more generalizable across several kinds of problems.
            </p>
            
            <h4>2. Visualizing CNNs:</h4>
            <p>
                This is looking at the feature maps and etc learned by CNNs to understand what is going on.
            </p>
            <p>
                <ol type="a">
                    <li>Zeiler et al: Deconvolutional networks to understand the patterns that activate CNN units.</li>
                    
                    <li>Zhou et al: Networks that are taught scene recognition also learn object identification, and can do both in a single forward pass.</li>
                </ol>
            </p>
            <p>
                Weaknesses: They ignore fully connected layers which also contribute to classification.
            </p>
            <p>
                Improvement: This paper uses nets without fully connected layers and retain most of the performance.
            </p>
            <p>
                <ol type="a" start="3">
                    <li>Mahendran et al, Dosovitskiy et al: Reconstruct input images from feature maps. Also considers fully connected layers.</li>
                </ol>
            </p>
            <p>
                Weaknesses: Does not address the relevance of locales.
            </p>
        </section>

        <section>
            <h2>Class Activation Mapping</h2>
            <p>
                A CAM is a visual explanation of the relevance of input image portions for a classification. 
                To find the CAMs, they use a network without fully connected layers (which are bad for object detection emergent quality produced by conv layers). Below is a description of the CAM structure.
            </p>
            <figure>
                <img src="https://github.com/user-attachments/assets/c6504176-a36c-4cc1-848a-15703a81608a" alt="CAM structure diagram" style="max-width: 600px; width: 100%;">
            </figure>
            <p>
                Intuitively, from this description, we can see how GAP would be better than GMP, as GAP captures its values from the feature map values for all the pixels, and GMP would pick just the most important pixel. It makes one wonder why Oquab et al ever decided on GMP in the first place.
            </p>
        </section>

        <section>
            <h2>Weakly Supervised Object Localization</h2>
            <p>
                Here, the paper evaluates CAM using the popular ILSVRC 2014 benchmark dataset and the popular CNNs: AlexNet, VGGnet and GoogLeNet but they remove the feed-forward layers and add a GAP layer at the end. 
                They perform four kinds of evaluations:
            </p>
            
            <h4>1. Classification:</h4>
            <p>
                Compare the original AlexNet, VGGnet and GoogleLeNet to understand the effects of removing the feed forward layers.
            </p>
            <p>
                There are small gains in error (1 to 2%) from losing the feed-forward layers. AlexNet shows more considerable suffering with its top-1 error going from 42.6 to 51.1. To compensate, they added in a few more convolutional layers and the error dropped to 44.9.
            </p>
            
            <h4>2. Localization:</h4>
            <p>
                Compare with the original GoogLeNet, and NIN (network in network). They also compare CAM with back-propagation.
            </p>
            <p>
                They generate bounding boxes from the CAM heatmaps by segmenting regions where the CAM values are 20% of the maximum CAM value. Then they take the largest connected bounding box.
            </p>
            <p>
                The GAP networks outperform all the classical ones with the GoogLeNet-GAP achieving the lowest error rate. This is remarkable considering that they did not train their GAP models for localization, just classification, but just drawing bounding boxes from the calculated CAMs would rival localization-trained models.
            </p>
            
            <h4>3. Global Average Pooling v. Global Max Pooling:</h4>
            <p>
                The GoogLeNet-GAP outperforms the GoogLeNet GMP.
            </p>
            
            <h4>4. Weakly v. Fully Supervised Methods:</h4>
            <p>
                The fully supervised methods outperform the weak. They note they still have work to be done here.
            </p>
        </section>

        <section>
            <h2>Deep Features for Generic Localization</h2>
            <p>
                Even though CNNs are trained on specific tasks like classifying the ImageNet database, we have found that that their deeper layers learn universal patterns which are transferable to other tasks. This is also true for the GAP-CNNs in this paper, with the added benefit that these higher levels also learn useful discrimination.
            </p>
            <p>
                To get the final softmax weights, they collect the output of the GAP layer from GoogleNet-GAP, a vector of averaged feature maps, and feed it as inputs to an SVM. Then they test this against AlexNet's fc7 features across a handful of vision tasks and eight datasets. Fine-grained recognition features the classification of 200 bird species and their GoogleNet-GAP features performs comparably with existing approaches.
            </p>
            <p>
                It also performs well on pattern discovery tasks which include discovering informative objects in scenes, recognizing concepts (like, view out of window) in images, text detection in images, finding the answer to a visual question (where are the cows) within images where it performs with an accuracy of 55.89%.
            </p>
        </section>

        <section>
            <h2>Visualizing Class-Specific Units</h2>
            <p>
                This paper goes on to visualize the class-specific units of a CNN using the ILSVRC and the Places datasets. Class-specific units are parts of the CNNs (group of neurons, kernels) that recognize textures and materials at low-level layers and objects and scenes at higher-level layers. We can see that the units detecting flowers and the units detecting grass are important to garden, inferring that CNNs, in a way, learn a bag of words to make predictions-- where each word is a discriminative class specific unit.
            </p>
        </section>

        <section>
            <h2>Reflection</h2>
            
            <h4>What is this paper about?</h4>
            <p>
                Visualization of the importance of input image portions for classifications, achieved by a network with only convolutional layers and topped with a global average pooling layer.
            </p>
            
            <h4>What are the strengths?</h4>
            <p>
                <ol>
                    <li>Thorough literature review</li>
                    <li>Thorough evaluation of technique</li>
                    <li>Good visualizations, and tables</li>
                    <li>Breaking new ground</li>
                </ol>
            </p>
            
            <h4>What are the weaknesses?</h4>
            <p>
                <ol>
                    <li>The feed-forward layers on models have to be removed before this technique can be applied.</li>
                    <li>Their description of CAM was difficult to follow.</li>
                    <li>Fully supervised methods still outperform their weakly supervised one.</li>
                </ol>
            </p>
            
            <h4>What are some significant follow up work from this paper? How do they differ from this paper?</h4>
            <p>
                <ol>
                    <li>Grad-CAM: Why did you say that?, 2016<br>
                    Introduces gradient-weighted CAMs for more transparency</li>
                    
                    <li>Eigen-CAM: Class Activation Map using Principal Components, 2020<br>
                    Uses PCA to more simply and more intuitively calculate CAMs</li>
                    
                    <li>Relevance-cam: Your model already knows where to look, 2021<br>
                    Works on using intermediate convolutional layers along with the final one</li>
                </ol>
            </p>
        </section>

        <a href="../index.html" class="back-link">← Back to all papers</a>
    </main>
    
    <footer>
        <p>© 2025 Sike Ogieva | Summary created for Interpretability of Deep Neural Networks, Spring 2025</p>
    </footer>

    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</body>
</html>